{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kafka-python\n",
      "  Using cached kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
      "Installing collected packages: kafka-python\n",
      "Successfully installed kafka-python-2.0.2\n",
      "Collecting snakeviz\n",
      "  Using cached snakeviz-2.1.1-py2.py3-none-any.whl (282 kB)\n",
      "Requirement already satisfied: tornado>=2.0 in /opt/conda/lib/python3.10/site-packages (from snakeviz) (6.1)\n",
      "Installing collected packages: snakeviz\n",
      "Successfully installed snakeviz-2.1.1\n",
      "Collecting memory_profiler\n",
      "  Using cached memory_profiler-0.60.0-py3-none-any.whl\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from memory_profiler) (5.9.2)\n",
      "Installing collected packages: memory_profiler\n",
      "Successfully installed memory_profiler-0.60.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip install snakeviz\n",
    "#!pip install memray # not for notebooks?\n",
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import cProfile\n",
    "import timeit\n",
    "\n",
    "%load_ext snakeviz\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Producer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def connect_kafka_producer(servers):\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print(f'Message published successfully to topic: {topic_name}.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "def produce_xy(producer, topic_name, sleep_hz):\n",
    "    with open('gen2_Heizungsdaten.csv') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            print(f'index {i}')\n",
    "            message = json.dumps({'data': str(line)})\n",
    "            print(message)\n",
    "            publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
    "            time.sleep(sleep_hz)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0\n",
      "{\"data\": \"2021-08-30 23:37:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.5,14.5,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.0,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 1\n",
      "{\"data\": \"2021-08-30 23:38:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.5,14.6,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 2\n",
      "{\"data\": \"2021-08-30 23:39:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.5,14.5,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.0,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 3\n",
      "{\"data\": \"2021-08-30 23:40:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.4,14.5,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 4\n",
      "{\"data\": \"2021-08-30 23:41:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.7,20.1,18.6,19.0,18.1,13.5,14.5,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.0,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 5\n",
      "{\"data\": \"2021-08-30 23:42:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.5,14.6,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 6\n",
      "{\"data\": \"2021-08-30 23:43:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.5,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 7\n",
      "{\"data\": \"2021-08-30 23:44:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.5,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 8\n",
      "{\"data\": \"2021-08-30 23:45:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,18.1,13.5,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 9\n",
      "{\"data\": \"2021-08-30 23:46:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,18.1,13.6,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 10\n",
      "{\"data\": \"2021-08-30 23:47:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,18.1,13.6,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 11\n",
      "{\"data\": \"2021-08-30 23:48:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.6,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 12\n",
      "{\"data\": \"2021-08-30 23:49:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.7,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 13\n",
      "{\"data\": \"2021-08-30 23:50:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.7,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 14\n",
      "{\"data\": \"2021-08-30 23:51:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,18.1,13.6,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 15\n",
      "{\"data\": \"2021-08-30 23:52:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,18.1,13.6,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 16\n",
      "{\"data\": \"2021-08-30 23:53:55,1,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,18.1,13.6,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 17\n",
      "{\"data\": \"2021-08-30 23:54:55,1,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,-27.5,46.9,15.6,20.1,18.6,19.0,17.9,13.5,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 18\n",
      "{\"data\": \"2021-08-30 23:55:55,1,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,18.1,13.5,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 19\n",
      "{\"data\": \"2021-08-30 23:56:55,1,0,0,0,0,1,0,0,0,1,0,0,0,1,0,0,1,0,-27.5,46.8,15.5,20.1,18.6,19.0,18.1,13.6,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 20\n",
      "{\"data\": \"2021-08-30 23:57:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,18.1,13.6,14.7,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 21\n",
      "{\"data\": \"2021-08-30 23:58:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.5,20.1,18.6,19.0,17.9,13.6,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.2,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 22\n",
      "{\"data\": \"2021-08-30 23:59:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.6,20.1,18.6,19.0,17.9,13.8,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.0,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 23\n",
      "{\"data\": \"2021-08-31 00:00:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.5,20.1,18.6,19.0,18.1,13.8,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.0,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n",
      "index 24\n",
      "{\"data\": \"2021-08-31 00:01:55,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0,0,1,0,-27.5,46.8,15.5,20.1,18.6,19.0,18.1,13.7,14.8,22.0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,155,155,155,155,155,0,0,0,0,0.4,1.1,0,0\\n\"}\n",
      "Message published successfully to topic: data_gen2.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m producer2 \u001b[38;5;241m=\u001b[39m connect_kafka_producer(server2)\n\u001b[1;32m      8\u001b[0m hz \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mproduce_xy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproducer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhz\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 30\u001b[0m, in \u001b[0;36mproduce_xy\u001b[0;34m(producer, topic_name, sleep_hz)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(message)\n\u001b[1;32m     29\u001b[0m publish_message(producer, topic_name, \u001b[38;5;28mstr\u001b[39m(uuid\u001b[38;5;241m.\u001b[39muuid4()), message)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_hz\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 2\n",
    "produce_xy(producer2, topic, hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Producer 2 Zeit Messungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def connect_kafka_producer(servers):\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        #print(f'Message published successfully to topic: {topic_name}.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "def produce_xy(producer, topic_name, sleep_hz, stop_by=5):\n",
    "    with open('gen2_Heizungsdaten.csv') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            #print(f'index {i}')\n",
    "            message = json.dumps({'data': str(line)})\n",
    "            #print(message)\n",
    "            publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
    "            time.sleep(sleep_hz)            \n",
    "            \n",
    "            # stop while loop for time measurement\n",
    "            if i > stop_by:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Laufzeit Messungen mit Timeit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.6 ms ± 2.29 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "#%snakeviz producer2 = connect_kafka_producer(server2)\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%timeit produce_xy(producer2, topic, hz, stop_by=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**B. Messungen mit SnakeViz**  \n",
    "Analyse Verbindungsaufbau Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "#%snakeviz producer2 = connect_kafka_producer(server2)\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "produce_xy(producer2, topic, hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Analyse Producer2 (data_generator)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         16385 function calls in 0.186 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "      412    0.140    0.000    0.140    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "      206    0.019    0.000    0.019    0.000 {method 'sendall' of '_socket.socket' objects}\n",
       "        1    0.002    0.002    0.186    0.186 3180120891.py:22(produce_xy)\n",
       "      102    0.002    0.000    0.002    0.000 default.py:36(murmur2)\n",
       "      102    0.002    0.000    0.024    0.000 kafka.py:538(send)\n",
       "      102    0.001    0.000    0.008    0.000 record_accumulator.py:200(append)\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
       "      102    0.001    0.000    0.001    0.000 encoder.py:204(iterencode)\n",
       "      102    0.001    0.000    0.001    0.000 legacy_records.py:391(_encode_msg)\n",
       "      102    0.001    0.000    0.003    0.000 legacy_records.py:352(append)\n",
       "      102    0.001    0.000    0.001    0.000 uuid.py:138(__init__)\n",
       "      102    0.001    0.000    0.155    0.002 kafka.py:624(flush)\n",
       "      102    0.001    0.000    0.138    0.001 record_accumulator.py:520(await_flush_completion)\n",
       "      103    0.001    0.000    0.141    0.001 threading.py:288(wait)\n",
       "      103    0.001    0.000    0.001    0.000 threading.py:236(__init__)\n",
       "      102    0.001    0.000    0.004    0.000 record_accumulator.py:57(try_append)\n",
       "      102    0.001    0.000    0.001    0.000 {built-in method posix.urandom}\n",
       "      206    0.000    0.000    0.019    0.000 client_async.py:929(wakeup)\n",
       "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
       "      102    0.000    0.000    0.178    0.002 3180120891.py:11(publish_message)\n",
       "      102    0.000    0.000    0.002    0.000 uuid.py:713(uuid4)\n",
       "      102    0.000    0.000    0.000    0.000 uuid.py:279(__str__)\n",
       "      102    0.000    0.000    0.001    0.000 encoder.py:182(encode)\n",
       "      103    0.000    0.000    0.141    0.001 threading.py:589(wait)\n",
       "      102    0.000    0.000    0.003    0.000 kafka.py:716(_partition)\n",
       "      102    0.000    0.000    0.006    0.000 kafka.py:664(_wait_on_metadata)\n",
       "      204    0.000    0.000    0.000    0.000 record_accumulator.py:29(decrement)\n",
       "      102    0.000    0.000    0.001    0.000 future.py:11(__init__)\n",
       "      102    0.000    0.000    0.001    0.000 future.py:32(__init__)\n",
       "      204    0.000    0.000    0.000    0.000 record_accumulator.py:24(increment)\n",
       "      102    0.000    0.000    0.000    0.000 memory_records.py:118(__init__)\n",
       "      205    0.000    0.000    0.000    0.000 cluster.py:106(partitions_for_topic)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method time.sleep}\n",
       "      102    0.000    0.000    0.001    0.000 record_accumulator.py:39(__init__)\n",
       "      204    0.000    0.000    0.000    0.000 legacy_records.py:486(record_size)\n",
       "      102    0.000    0.000    0.001    0.000 cluster.py:119(available_partitions_for_topic)\n",
       "      102    0.000    0.000    0.000    0.000 record_accumulator.py:588(all)\n",
       "      306    0.000    0.000    0.000    0.000 {built-in method _struct.pack_into}\n",
       "      206    0.000    0.000    0.020    0.000 sender.py:331(wakeup)\n",
       "     1128    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "      102    0.000    0.000    0.001    0.000 kafka.py:529(_estimate_size_in_bytes)\n",
       "      102    0.000    0.000    0.002    0.000 __init__.py:183(dumps)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method binascii.crc32}\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "     1428    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "      103    0.000    0.000    0.001    0.000 threading.py:545(__init__)\n",
       "      102    0.000    0.000    0.002    0.000 default.py:15(__call__)\n",
       "      204    0.000    0.000    0.000    0.000 __init__.py:1455(debug)\n",
       "      102    0.000    0.000    0.000    0.000 buffer.py:38(allocate)\n",
       "      205    0.000    0.000    0.000    0.000 future.py:12(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:503(estimate_size_in_bytes)\n",
       "      102    0.000    0.000    0.137    0.001 future.py:26(wait)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:346(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 util.py:131(calc_crc32)\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:276(_acquire_restore)\n",
       "      102    0.000    0.000    0.000    0.000 record_accumulator.py:580(add)\n",
       "      207    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "      308    0.000    0.000    0.000    0.000 kafka.py:454(__getattr__)\n",
       "      102    0.000    0.000    0.003    0.000 memory_records.py:137(append)\n",
       "      103    0.000    0.000    0.000    0.000 future.py:48(add_callback)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "      204    0.000    0.000    0.000    0.000 __init__.py:1724(isEnabledFor)\n",
       "      612    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "      102    0.000    0.000    0.000    0.000 six.py:592(iteritems)\n",
       "      102    0.000    0.000    0.000    0.000 {method 'pack_into' of '_struct.Struct' objects}\n",
       "      102    0.000    0.000    0.000    0.000 kafka.py:521(_max_usable_produce_magic)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:521(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 future.py:22(failed)\n",
       "      102    0.000    0.000    0.000    0.000 cluster.py:131(<listcomp>)\n",
       "      104    0.000    0.000    0.000    0.000 threading.py:267(__exit__)\n",
       "      102    0.000    0.000    0.000    0.000 sender.py:173(add_topic)\n",
       "      204    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:479(size_in_bytes)\n",
       "      409    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "      204    0.000    0.000    0.000    0.000 legacy_records.py:495(record_overhead)\n",
       "      102    0.000    0.000    0.000    0.000 memory_records.py:176(is_full)\n",
       "      102    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
       "      102    0.000    0.000    0.000    0.000 {method 'extend' of 'bytearray' objects}\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:279(_is_owned)\n",
       "      104    0.000    0.000    0.000    0.000 threading.py:264(__enter__)\n",
       "      103    0.000    0.000    0.000    0.000 future.py:57(add_errback)\n",
       "      102    0.000    0.000    0.000    0.000 kafka.py:651(_ensure_valid_record_size)\n",
       "      102    0.000    0.000    0.000    0.000 record_accumulator.py:513(begin_flush)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x55bcb0433540}\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "        1    0.000    0.000    0.186    0.186 {built-in method builtins.exec}\n",
       "      308    0.000    0.000    0.000    0.000 kafka.py:455(<lambda>)\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:273(_release_save)\n",
       "      102    0.000    0.000    0.000    0.000 {method 'count' of 'list' objects}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:472(size)\n",
       "      204    0.000    0.000    0.000    0.000 kafka.py:709(_serialize)\n",
       "      206    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "      204    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "      102    0.000    0.000    0.000    0.000 kafka.py:593(<genexpr>)\n",
       "      205    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:539(timestamp)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:535(size)\n",
       "      103    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "      102    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "      102    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "      104    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
       "      102    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
       "      102    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:531(crc)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:527(offset)\n",
       "        3    0.000    0.000    0.000    0.000 codecs.py:319(decode)\n",
       "      103    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
       "        1    0.000    0.000    0.186    0.186 <string>:1(<module>)\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}\n",
       "        1    0.000    0.000    0.000    0.000 cluster.py:185(request_update)\n",
       "        1    0.000    0.000    0.000    0.000 future.py:66(add_both)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 threading.py:579(clear)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 threading.py:553(is_set)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%prun produce_xy(producer2, topic, hz, stop_by=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '/tmp/tmpebkz0enk'.\n",
      "Embedding SnakeViz in this document...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe id='snakeviz-fb935b58-5479-11ed-b49e-0242ac130003' frameborder=0 seamless width='100%' height='1000'></iframe>\n",
       "<script>document.getElementById(\"snakeviz-fb935b58-5479-11ed-b49e-0242ac130003\").setAttribute(\"src\", \"http://\" + document.location.hostname + \":8080/snakeviz/%2Ftmp%2Ftmpebkz0enk\")</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%snakeviz produce_xy(producer2, topic, hz, stop_by=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse Memory Nutzung ([memory_profiler](https://ipython-books.github.io/44-profiling-the-memory-usage-of-your-code-with-memory_profiler/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 109.91 MiB, increment: -9.77 MiB\n"
     ]
    }
   ],
   "source": [
    "# function memory measurement,function uses ca. 1\n",
    "#%memit?\n",
    "%memit  produce_xy(producer2, topic, hz, stop_by=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mprun_memory_producer2.py\n"
     ]
    }
   ],
   "source": [
    "%%file mprun_memory_producer2.py \n",
    "from memory_profiler import profile\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "@profile\n",
    "def connect_kafka_producer(servers):\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "@profile\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        #print(f'Message published successfully to topic: {topic_name}.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "@profile\n",
    "def produce_xy(producer, topic_name, sleep_hz, stop_by=5):\n",
    "    with open('gen2_Heizungsdaten.csv') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            # stop while loop for time measurement\n",
    "            if i > stop_by:\n",
    "                break\n",
    "            \n",
    "            #print(f'index {i}')\n",
    "            message = json.dumps({'data': str(line)})\n",
    "            #print(message)\n",
    "            publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
    "            time.sleep(sleep_hz)            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jovyan/data/mprun_memory_producer2.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7    110.4 MiB    110.4 MiB           1   @profile\n",
      "     8                                         def connect_kafka_producer(servers):\n",
      "     9    110.4 MiB      0.0 MiB           1       _producer = None\n",
      "    10    110.4 MiB      0.0 MiB           1       try:\n",
      "    11    110.5 MiB      0.1 MiB           1           _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
      "    12                                             except Exception as ex:\n",
      "    13                                                 print('Exception while connecting Kafka')\n",
      "    14                                                 print(str(ex))\n",
      "    15                                             finally:\n",
      "    16    110.5 MiB      0.0 MiB           1           return _producer\n",
      "\n",
      "\n",
      "Filename: /home/jovyan/data/mprun_memory_producer2.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    18    110.5 MiB    110.5 MiB           1   @profile\n",
      "    19                                         def publish_message(producer_instance, topic_name, key, value):\n",
      "    20    110.5 MiB      0.0 MiB           1       try:\n",
      "    21    110.5 MiB      0.0 MiB           1           key_bytes = bytes(key, encoding='utf-8')\n",
      "    22    110.5 MiB      0.0 MiB           1           value_bytes = bytes(value, encoding='utf-8')\n",
      "    23    110.5 MiB      0.0 MiB           1           producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
      "    24    110.5 MiB      0.0 MiB           1           producer_instance.flush()\n",
      "    25                                                 #print(f'Message published successfully to topic: {topic_name}.')\n",
      "    26                                             except Exception as ex:\n",
      "    27                                                 print('Exception in publishing message')\n",
      "    28                                                 print(str(ex))\n",
      "\n",
      "\n",
      "Filename: /home/jovyan/data/mprun_memory_producer2.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    30    110.5 MiB    110.5 MiB           1   @profile\n",
      "    31                                         def produce_xy(producer, topic_name, sleep_hz, stop_by=5):\n",
      "    32    110.5 MiB      0.0 MiB           2       with open('gen2_Heizungsdaten.csv') as f:\n",
      "    33    110.5 MiB      0.0 MiB           1           next(f)\n",
      "    34    110.5 MiB      0.0 MiB           2           for i, line in enumerate(f):\n",
      "    35                                                     # stop while loop for time measurement\n",
      "    36    110.5 MiB      0.0 MiB           2               if i > stop_by:\n",
      "    37    110.5 MiB      0.0 MiB           1                   break\n",
      "    38                                                     \n",
      "    39                                                     #print(f'index {i}')\n",
      "    40    110.5 MiB      0.0 MiB           1               message = json.dumps({'data': str(line)})\n",
      "    41                                                     #print(message)\n",
      "    42    110.5 MiB      0.0 MiB           1               publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
      "    43    110.5 MiB      0.0 MiB           1               time.sleep(sleep_hz)            \n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: /opt/conda/lib/python3.10/site-packages/memory_profiler.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
       "=============================================================\n",
       "  1183    110.5 MiB    110.5 MiB           2               @wraps(wrapped=func)\n",
       "  1184                                                     def wrapper(*args, **kwargs):\n",
       "  1185    110.5 MiB      0.0 MiB           2                   prof = get_prof()\n",
       "  1186    110.5 MiB      0.0 MiB           2                   val = prof(func)(*args, **kwargs)\n",
       "  1187    110.5 MiB      0.0 MiB           2                   show_results_bound(prof)\n",
       "  1188    110.5 MiB      0.0 MiB           2                   return val"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mprun_memory_producer2 import *\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "\n",
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%mprun -f produce_xy produce_xy(producer2, topic, hz, stop_by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
