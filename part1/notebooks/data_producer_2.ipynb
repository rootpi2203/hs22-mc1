{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kafka-python in /opt/conda/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: snakeviz in /opt/conda/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: tornado>=2.0 in /opt/conda/lib/python3.10/site-packages (from snakeviz) (6.1)\n",
      "Requirement already satisfied: memory_profiler in /opt/conda/lib/python3.10/site-packages (0.60.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from memory_profiler) (5.9.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip install snakeviz\n",
    "#!pip install memray # not for notebooks?\n",
    "!pip install memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "import cProfile\n",
    "import timeit\n",
    "\n",
    "%load_ext snakeviz\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Producer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def connect_kafka_producer(servers):\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print(f'Message published successfully to topic: {topic_name}.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "def produce_xy(producer, topic_name, sleep_hz):\n",
    "    with open('gen2_Heizungsdaten.csv') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            print(f'index {i}')\n",
    "            message = json.dumps({'data': str(line)})\n",
    "            print(message)\n",
    "            publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
    "            time.sleep(sleep_hz)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#server1 = 'broker1:9093'\n",
    "#server2 = 'broker2:9095'\n",
    "#server3 = 'broker3:9097'\n",
    "#topic = \"data_gen2\"\n",
    "\n",
    "#producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "#hz = 2\n",
    "#produce_xy(producer2, topic, hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Producer 2 Zeit Messungen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def connect_kafka_producer(servers):\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        #print(f'Message published successfully to topic: {topic_name}.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "def produce_xy(producer, topic_name, sleep_hz, stop_by=5):\n",
    "    with open('gen2_Heizungsdaten.csv') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            #print(f'index {i}')\n",
    "            message = json.dumps({'data': str(line)})\n",
    "            #print(message)\n",
    "            publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
    "            time.sleep(sleep_hz)            \n",
    "            \n",
    "            # stop while loop for time measurement\n",
    "            if i > stop_by:\n",
    "                break\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. Laufzeit Messungen mit Timeit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.6 ms ± 1.93 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "#%snakeviz producer2 = connect_kafka_producer(server2)\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%timeit produce_xy(producer2, topic, hz, stop_by=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**B. Messungen mit SnakeViz**  \n",
    "Analyse Verbindungsaufbau Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "#%snakeviz producer2 = connect_kafka_producer(server2)\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "produce_xy(producer2, topic, hz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Analyse Producer2 (data_generator)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "text/plain": [
       "         16385 function calls in 0.146 seconds\n",
       "\n",
       "   Ordered by: internal time\n",
       "\n",
       "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
       "      412    0.090    0.000    0.090    0.000 {method 'acquire' of '_thread.lock' objects}\n",
       "      206    0.024    0.000    0.024    0.000 {method 'sendall' of '_socket.socket' objects}\n",
       "      102    0.002    0.000    0.002    0.000 default.py:36(murmur2)\n",
       "      102    0.002    0.000    0.027    0.000 kafka.py:538(send)\n",
       "        1    0.002    0.002    0.146    0.146 3180120891.py:22(produce_xy)\n",
       "        1    0.001    0.001    0.001    0.001 {built-in method io.open}\n",
       "      102    0.001    0.000    0.010    0.000 record_accumulator.py:200(append)\n",
       "      102    0.001    0.000    0.001    0.000 encoder.py:204(iterencode)\n",
       "      102    0.001    0.000    0.088    0.001 record_accumulator.py:520(await_flush_completion)\n",
       "      102    0.001    0.000    0.002    0.000 legacy_records.py:391(_encode_msg)\n",
       "      103    0.001    0.000    0.091    0.001 threading.py:288(wait)\n",
       "      102    0.001    0.000    0.003    0.000 legacy_records.py:352(append)\n",
       "      102    0.001    0.000    0.001    0.000 uuid.py:138(__init__)\n",
       "      102    0.001    0.000    0.110    0.001 kafka.py:624(flush)\n",
       "      102    0.001    0.000    0.005    0.000 record_accumulator.py:57(try_append)\n",
       "      102    0.001    0.000    0.001    0.000 {built-in method posix.urandom}\n",
       "      103    0.001    0.000    0.001    0.000 threading.py:236(__init__)\n",
       "      102    0.001    0.000    0.137    0.001 3180120891.py:11(publish_message)\n",
       "      206    0.001    0.000    0.024    0.000 client_async.py:929(wakeup)\n",
       "      103    0.000    0.000    0.091    0.001 threading.py:589(wait)\n",
       "      102    0.000    0.000    0.000    0.000 uuid.py:279(__str__)\n",
       "      102    0.000    0.000    0.002    0.000 uuid.py:713(uuid4)\n",
       "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_io._IOBase' objects}\n",
       "      102    0.000    0.000    0.001    0.000 future.py:32(__init__)\n",
       "      102    0.000    0.000    0.002    0.000 encoder.py:182(encode)\n",
       "      102    0.000    0.000    0.004    0.000 kafka.py:716(_partition)\n",
       "      102    0.000    0.000    0.002    0.000 record_accumulator.py:39(__init__)\n",
       "      102    0.000    0.000    0.006    0.000 kafka.py:664(_wait_on_metadata)\n",
       "      102    0.000    0.000    0.001    0.000 future.py:11(__init__)\n",
       "      102    0.000    0.000    0.001    0.000 memory_records.py:118(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method time.sleep}\n",
       "      306    0.000    0.000    0.000    0.000 {built-in method _struct.pack_into}\n",
       "      205    0.000    0.000    0.000    0.000 cluster.py:106(partitions_for_topic)\n",
       "      204    0.000    0.000    0.000    0.000 record_accumulator.py:29(decrement)\n",
       "      204    0.000    0.000    0.000    0.000 record_accumulator.py:24(increment)\n",
       "      102    0.000    0.000    0.001    0.000 cluster.py:119(available_partitions_for_topic)\n",
       "      204    0.000    0.000    0.000    0.000 legacy_records.py:486(record_size)\n",
       "      205    0.000    0.000    0.000    0.000 future.py:12(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method binascii.crc32}\n",
       "      102    0.000    0.000    0.001    0.000 kafka.py:529(_estimate_size_in_bytes)\n",
       "        1    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
       "      102    0.000    0.000    0.000    0.000 record_accumulator.py:588(all)\n",
       "      102    0.000    0.000    0.000    0.000 {method 'pack_into' of '_struct.Struct' objects}\n",
       "     1128    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
       "      102    0.000    0.000    0.002    0.000 __init__.py:183(dumps)\n",
       "      206    0.000    0.000    0.024    0.000 sender.py:331(wakeup)\n",
       "      103    0.000    0.000    0.001    0.000 threading.py:545(__init__)\n",
       "      102    0.000    0.000    0.001    0.000 legacy_records.py:503(estimate_size_in_bytes)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
       "      102    0.000    0.000    0.000    0.000 buffer.py:38(allocate)\n",
       "     1428    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
       "      102    0.000    0.000    0.002    0.000 default.py:15(__call__)\n",
       "      102    0.000    0.000    0.087    0.001 future.py:26(wait)\n",
       "      204    0.000    0.000    0.000    0.000 __init__.py:1455(debug)\n",
       "      103    0.000    0.000    0.000    0.000 future.py:48(add_callback)\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:276(_acquire_restore)\n",
       "      102    0.000    0.000    0.000    0.000 record_accumulator.py:580(add)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:346(__init__)\n",
       "      207    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
       "      308    0.000    0.000    0.000    0.000 kafka.py:454(__getattr__)\n",
       "      102    0.000    0.000    0.000    0.000 six.py:592(iteritems)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
       "      204    0.000    0.000    0.000    0.000 __init__.py:1724(isEnabledFor)\n",
       "      102    0.000    0.000    0.000    0.000 util.py:131(calc_crc32)\n",
       "      102    0.000    0.000    0.003    0.000 memory_records.py:137(append)\n",
       "      104    0.000    0.000    0.000    0.000 threading.py:267(__exit__)\n",
       "      612    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:521(__init__)\n",
       "      102    0.000    0.000    0.000    0.000 future.py:22(failed)\n",
       "      102    0.000    0.000    0.000    0.000 cluster.py:131(<listcomp>)\n",
       "      204    0.000    0.000    0.000    0.000 legacy_records.py:495(record_overhead)\n",
       "      102    0.000    0.000    0.000    0.000 sender.py:173(add_topic)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:479(size_in_bytes)\n",
       "      102    0.000    0.000    0.000    0.000 kafka.py:521(_max_usable_produce_magic)\n",
       "      409    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
       "      204    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
       "      102    0.000    0.000    0.000    0.000 {method 'extend' of 'bytearray' objects}\n",
       "      102    0.000    0.000    0.000    0.000 memory_records.py:176(is_full)\n",
       "      104    0.000    0.000    0.000    0.000 threading.py:264(__enter__)\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:279(_is_owned)\n",
       "      102    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
       "      102    0.000    0.000    0.000    0.000 record_accumulator.py:513(begin_flush)\n",
       "      102    0.000    0.000    0.000    0.000 kafka.py:593(<genexpr>)\n",
       "      102    0.000    0.000    0.000    0.000 kafka.py:651(_ensure_valid_record_size)\n",
       "      308    0.000    0.000    0.000    0.000 kafka.py:455(<lambda>)\n",
       "      103    0.000    0.000    0.000    0.000 future.py:57(add_errback)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x55afd9d54540}\n",
       "      103    0.000    0.000    0.000    0.000 threading.py:273(_release_save)\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
       "      102    0.000    0.000    0.000    0.000 {method 'count' of 'list' objects}\n",
       "        1    0.000    0.000    0.146    0.146 {built-in method builtins.exec}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:472(size)\n",
       "      204    0.000    0.000    0.000    0.000 kafka.py:709(_serialize)\n",
       "      205    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
       "      204    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
       "      206    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:535(size)\n",
       "      103    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
       "      102    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
       "        3    0.000    0.000    0.000    0.000 codecs.py:319(decode)\n",
       "      104    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:531(crc)\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:527(offset)\n",
       "      102    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
       "      102    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
       "      102    0.000    0.000    0.000    0.000 legacy_records.py:539(timestamp)\n",
       "      102    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
       "      102    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
       "        3    0.000    0.000    0.000    0.000 {built-in method _codecs.utf_8_decode}\n",
       "      103    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
       "        1    0.000    0.000    0.146    0.146 <string>:1(<module>)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:309(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 cluster.py:185(request_update)\n",
       "        1    0.000    0.000    0.000    0.000 future.py:66(add_both)\n",
       "        1    0.000    0.000    0.000    0.000 threading.py:579(clear)\n",
       "        1    0.000    0.000    0.000    0.000 codecs.py:260(__init__)\n",
       "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
       "        1    0.000    0.000    0.000    0.000 threading.py:553(is_set)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%prun produce_xy(producer2, topic, hz, stop_by=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "*** Profile stats marshalled to file '/tmp/tmp8c93py5w'.\n",
      "Embedding SnakeViz in this document...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<iframe id='snakeviz-363a096c-5477-11ed-a64b-0242ac120003' frameborder=0 seamless width='100%' height='1000'></iframe>\n",
       "<script>document.getElementById(\"snakeviz-363a096c-5477-11ed-a64b-0242ac120003\").setAttribute(\"src\", \"http://\" + document.location.hostname + \":8080/snakeviz/%2Ftmp%2Ftmp8c93py5w\")</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%snakeviz produce_xy(producer2, topic, hz, stop_by=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse Memory Nutzung ([memory_profiler](https://ipython-books.github.io/44-profiling-the-memory-usage-of-your-code-with-memory_profiler/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 112.49 MiB, increment: -7.91 MiB\n"
     ]
    }
   ],
   "source": [
    "# function memory measurement,function uses ca. 1\n",
    "#%memit?\n",
    "%memit  produce_xy(producer2, topic, hz, stop_by=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mprun_memory_producer2.py\n"
     ]
    }
   ],
   "source": [
    "%%file mprun_memory_producer2.py \n",
    "from memory_profiler import profile\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "@profile\n",
    "def connect_kafka_producer(servers):\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "@profile\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        #print(f'Message published successfully to topic: {topic_name}.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "@profile\n",
    "def produce_xy(producer, topic_name, sleep_hz, stop_by=5):\n",
    "    with open('gen2_Heizungsdaten.csv') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            #print(f'index {i}')\n",
    "            message = json.dumps({'data': str(line)})\n",
    "            #print(message)\n",
    "            publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
    "            time.sleep(sleep_hz)            \n",
    "            \n",
    "            # stop while loop for time measurement\n",
    "            if i > stop_by:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /home/jovyan/data/mprun_memory_producer2.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     7    112.9 MiB    112.9 MiB           1   @profile\n",
      "     8                                         def connect_kafka_producer(servers):\n",
      "     9    112.9 MiB      0.0 MiB           1       _producer = None\n",
      "    10    112.9 MiB      0.0 MiB           1       try:\n",
      "    11    113.1 MiB      0.1 MiB           1           _producer = KafkaProducer(bootstrap_servers=servers, api_version=(0, 10))\n",
      "    12                                             except Exception as ex:\n",
      "    13                                                 print('Exception while connecting Kafka')\n",
      "    14                                                 print(str(ex))\n",
      "    15                                             finally:\n",
      "    16    113.1 MiB      0.0 MiB           1           return _producer\n",
      "\n",
      "\n",
      "Filename: /home/jovyan/data/mprun_memory_producer2.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    18    113.1 MiB    113.1 MiB           1   @profile\n",
      "    19                                         def publish_message(producer_instance, topic_name, key, value):\n",
      "    20    113.1 MiB      0.0 MiB           1       try:\n",
      "    21    113.1 MiB      0.0 MiB           1           key_bytes = bytes(key, encoding='utf-8')\n",
      "    22    113.1 MiB      0.0 MiB           1           value_bytes = bytes(value, encoding='utf-8')\n",
      "    23    113.1 MiB      0.0 MiB           1           producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
      "    24    113.1 MiB      0.0 MiB           1           producer_instance.flush()\n",
      "    25                                                 #print(f'Message published successfully to topic: {topic_name}.')\n",
      "    26                                             except Exception as ex:\n",
      "    27                                                 print('Exception in publishing message')\n",
      "    28                                                 print(str(ex))\n",
      "\n",
      "\n",
      "Filename: /home/jovyan/data/mprun_memory_producer2.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    18    113.1 MiB    113.1 MiB           1   @profile\n",
      "    19                                         def publish_message(producer_instance, topic_name, key, value):\n",
      "    20    113.1 MiB      0.0 MiB           1       try:\n",
      "    21    113.1 MiB      0.0 MiB           1           key_bytes = bytes(key, encoding='utf-8')\n",
      "    22    113.1 MiB      0.0 MiB           1           value_bytes = bytes(value, encoding='utf-8')\n",
      "    23    113.1 MiB      0.0 MiB           1           producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
      "    24    113.1 MiB      0.0 MiB           1           producer_instance.flush()\n",
      "    25                                                 #print(f'Message published successfully to topic: {topic_name}.')\n",
      "    26                                             except Exception as ex:\n",
      "    27                                                 print('Exception in publishing message')\n",
      "    28                                                 print(str(ex))\n",
      "\n",
      "\n",
      "Filename: /home/jovyan/data/mprun_memory_producer2.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    30    113.1 MiB    113.1 MiB           1   @profile\n",
      "    31                                         def produce_xy(producer, topic_name, sleep_hz, stop_by=5):\n",
      "    32    113.1 MiB      0.0 MiB           2       with open('gen2_Heizungsdaten.csv') as f:\n",
      "    33    113.1 MiB      0.0 MiB           1           next(f)\n",
      "    34    113.1 MiB      0.0 MiB           2           for i, line in enumerate(f):\n",
      "    35                                                     #print(f'index {i}')\n",
      "    36    113.1 MiB      0.0 MiB           2               message = json.dumps({'data': str(line)})\n",
      "    37                                                     #print(message)\n",
      "    38    113.1 MiB      0.1 MiB           2               publish_message(producer, topic_name, str(uuid.uuid4()), message)\n",
      "    39    113.1 MiB      0.0 MiB           2               time.sleep(sleep_hz)            \n",
      "    40                                                     \n",
      "    41                                                     # stop while loop for time measurement\n",
      "    42    113.1 MiB      0.0 MiB           2               if i > stop_by:\n",
      "    43    113.1 MiB      0.0 MiB           1                   break\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: /opt/conda/lib/python3.10/site-packages/memory_profiler.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
       "=============================================================\n",
       "  1183    113.1 MiB    113.1 MiB           3               @wraps(wrapped=func)\n",
       "  1184                                                     def wrapper(*args, **kwargs):\n",
       "  1185    113.1 MiB      0.0 MiB           3                   prof = get_prof()\n",
       "  1186    113.1 MiB      0.0 MiB           3                   val = prof(func)(*args, **kwargs)\n",
       "  1187    113.1 MiB      0.0 MiB           3                   show_results_bound(prof)\n",
       "  1188    113.1 MiB      0.0 MiB           3                   return val"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mprun_memory_producer2 import *\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "\n",
    "server1 = 'broker1:9093'\n",
    "server2 = 'broker2:9095'\n",
    "server3 = 'broker3:9097'\n",
    "topic = \"data_gen2\"\n",
    "\n",
    "producer2 = connect_kafka_producer(server2)\n",
    "\n",
    "hz = 0\n",
    "%mprun -f produce_xy produce_xy(producer2, topic, hz, stop_by=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
